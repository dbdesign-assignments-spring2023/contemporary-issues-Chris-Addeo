# Contemporary Issues In Data - Chris Adddeo, cda8682


# Youtube's User of Viewer Data to Influence their Political Leanings
In this document, I will be discussing two articles about how Youtube recommends political content and influences the direction of a user's ideological views. With Youtube having over 122 million daily users, the implications of such issues must be taken seriously, so I will present and discuss two opposing views on this important matter.


## Article 1
In The Verge's [How white supremacists are thriving on YouTube](https://www.theverge.com/2018/9/19/17876892/youtube-extremism-report-rebecca-lewis-data-society), major institutions like Youtube perpetuate right-wing extremism by amplifying conservative content. Although search engines benefit moderate to liberal phrasing, influencers are able to capitalize on this through emphasizing their relatability, “authenticity,” and relation to a larger counter culture. From there, they encourage viewers to adopt more radical ideologies over time, then introduce them to extremist figures. The report from the article has shown how these attempts to appear as "moderate" and "objective" are being exploited by users who fundamentally reject objectivity as a valid stance and push misinformation.

Youtube's recommendation system also encourages this behavior. Once you view a right-wing influencer's channel, more channel's with similar views start popping up, until you have over 80 channels full of slightly conservative to extremist content. It has been studied that this trend does not function the same for left-wing content, which is not boosted as much by search engines and recommendations. More concerning, some claim this is a societal issue that is being exploited. Many of these influencers recommend their extremist peers to their viewers, and the intensification of recommendation systems only strengthens the push into an ideological rabbit hole.


## Article 2
Contrary to the claims made in The Verge's piece on the issue, Forbes magazine's [The Amazing Ways YouTube Uses Artificial Intelligence And Machine Learning](https://www.forbes.com/sites/bernardmarr/2019/08/23/the-amazing-ways-youtube-uses-artificial-intelligence-and-machine-learning/?sh=10136a115852), found that over 25 million videos were removed in 2019. Said videos were violent, politically extremist, and broke Youtube's rules. Over 70% of the videos were removed before there were ever viewed. Considering that 300 hours content are uploaded every minute to the platform, Youtube is putting in a lot of effort to combatting extremism. In fact, they are being so strict that the algorithm sometimes pulls down trust-worthy news publishers after mistaking their videos as violent extremism.

Youtube is not removing objectionable content for moral or ethical reasons, but for the ability to continue selling ads for agencies and brands. Some claim that youtube does not care that they push audiences rightwards if it means they make a profit. The political extremism they do take down often comes from terrorist organizations. Most American-extremist content does not get taken down unless it has misinformation. Even though youtube removes a lot of content, they still push right-leaning content and this article does not dispute that - most respected publishers acknowledge this reality. Artificial intelligence does a great job at reducing the amount of harmful content that users consume, but it is still used as a tool to further conservative ideologies whether or not Youtube acknowledges it.


